{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8754762,"sourceType":"datasetVersion","datasetId":5259241},{"sourceId":8757843,"sourceType":"datasetVersion","datasetId":5261443},{"sourceId":8759809,"sourceType":"datasetVersion","datasetId":5262902},{"sourceId":8765391,"sourceType":"datasetVersion","datasetId":5266848},{"sourceId":8783550,"sourceType":"datasetVersion","datasetId":5280117},{"sourceId":8785959,"sourceType":"datasetVersion","datasetId":5281925}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-26T21:41:25.682470Z","iopub.execute_input":"2024-06-26T21:41:25.682796Z","iopub.status.idle":"2024-06-26T21:41:39.202326Z","shell.execute_reply.started":"2024-06-26T21:41:25.682771Z","shell.execute_reply":"2024-06-26T21:41:39.201265Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.3.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport wandb\nimport random\n\nimport math \n\nimport numpy as np\nimport pandas as pd\nimport re\n\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom nltk.translate.bleu_score import sentence_bleu ,SmoothingFunction \n\nfrom tokenizers import Tokenizer\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:41:39.204162Z","iopub.execute_input":"2024-06-26T21:41:39.204462Z","iopub.status.idle":"2024-06-26T21:41:49.021154Z","shell.execute_reply.started":"2024-06-26T21:41:39.204434Z","shell.execute_reply":"2024-06-26T21:41:49.020358Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/model","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:41:49.022174Z","iopub.execute_input":"2024-06-26T21:41:49.022644Z","iopub.status.idle":"2024-06-26T21:41:49.969053Z","shell.execute_reply.started":"2024-06-26T21:41:49.022611Z","shell.execute_reply":"2024-06-26T21:41:49.967977Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"2448fa0c4f7d7dcf19f87fbb3ce5a9743515b380\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:41:49.971487Z","iopub.execute_input":"2024-06-26T21:41:49.971792Z","iopub.status.idle":"2024-06-26T21:41:52.373823Z","shell.execute_reply.started":"2024-06-26T21:41:49.971762Z","shell.execute_reply":"2024-06-26T21:41:52.373020Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(\n    project=\"G2P\",\n    config={\n    \"BATCH_SIZE\":  512,\n    \"NUM_EPOHS\":  40,\n    \"LR\": 3e-4,\n    \"D_MODEL\":  512,\n    \"D_FF\": 2048,\n    \"NUM\": 3,\n    \"NUM_HEADS\" : 4,\n    \"RANDOM_STATE\": 42,\n    \"MAX_LEN\": 32,\n    \"SAVE_DIR\": \"/kaggle/working/model\",\n    \"BOS_TOKEN\": \"<bos>\",\n    \"EOS_TOKEN\": \"<eos>\",\n    \"PAD_TOKEN\": \"<pad>\",\n    'UNK_TOKEN': \"<unk>\"\n    },\n    name=\"обучение на 512_2048_33_44 без стресса\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:41:52.375059Z","iopub.execute_input":"2024-06-26T21:41:52.375850Z","iopub.status.idle":"2024-06-26T21:42:09.322821Z","shell.execute_reply.started":"2024-06-26T21:41:52.375815Z","shell.execute_reply":"2024-06-26T21:42:09.321793Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicitacom2018\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240626_214152-iwa93ate</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nicitacom2018/G2P/runs/iwa93ate' target=\"_blank\">обучение на 512_2048_33_44 без стресса</a></strong> to <a href='https://wandb.ai/nicitacom2018/G2P' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nicitacom2018/G2P' target=\"_blank\">https://wandb.ai/nicitacom2018/G2P</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nicitacom2018/G2P/runs/iwa93ate' target=\"_blank\">https://wandb.ai/nicitacom2018/G2P/runs/iwa93ate</a>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nicitacom2018/G2P/runs/iwa93ate?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7dd1e6d88670>"},"metadata":{}}]},{"cell_type":"code","source":"device  = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:09.324037Z","iopub.execute_input":"2024-06-26T21:42:09.324388Z","iopub.status.idle":"2024-06-26T21:42:09.391292Z","shell.execute_reply.started":"2024-06-26T21:42:09.324355Z","shell.execute_reply":"2024-06-26T21:42:09.390282Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"def pd_get_list(path):\n    df = pd.read_csv(path)\n    df = df.dropna()\n    word_list = df['word'].to_list()\n    phoneme_list = df['phoneme'].to_list()\n    return word_list, phoneme_list","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:09.392563Z","iopub.execute_input":"2024-06-26T21:42:09.392873Z","iopub.status.idle":"2024-06-26T21:42:09.403108Z","shell.execute_reply.started":"2024-06-26T21:42:09.392847Z","shell.execute_reply":"2024-06-26T21:42:09.402141Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"word_list_test , phoneme_list_test = pd_get_list(\"/kaggle/input/lexicin200k/test.csv\")\nword_list_train , phoneme_list_train = pd_get_list(\"/kaggle/input/lexicin200k/train.csv\")\nword_list_val , phoneme_list_val = pd_get_list(\"/kaggle/input/lexicin200k/val.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:09.404388Z","iopub.execute_input":"2024-06-26T21:42:09.405136Z","iopub.status.idle":"2024-06-26T21:42:09.815563Z","shell.execute_reply.started":"2024-06-26T21:42:09.405102Z","shell.execute_reply":"2024-06-26T21:42:09.814501Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class CreateDataset:\n    def __init__(self, word , phoneme, stress=True):\n        self.word = word\n        self.phoneme = phoneme\n        self.stress = stress\n        \n    def __getitem__(self, idx: int):\n        if  not(self.stress):\n            if any( phonem[-1] in '0123' for phonem in  self.phoneme[idx].split() ):\n                self.phoneme_ = re.sub(r'\\d+', '', self.phoneme[idx]).strip()\n                \n            return {\"input\": self.word[idx],\n                    \"label\": self.phoneme_}\n        \n        else:\n             return {\"input\": self.word[idx].strip(),\n                    \"label\": self.phoneme[idx]}\n            \n    \n    def __len__(self):\n        return len(self.word)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:09.817131Z","iopub.execute_input":"2024-06-26T21:42:09.817509Z","iopub.status.idle":"2024-06-26T21:42:09.833735Z","shell.execute_reply.started":"2024-06-26T21:42:09.817473Z","shell.execute_reply":"2024-06-26T21:42:09.832620Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def crate_dataset(word_list ,phoneme_list):\n    data_with_stress = CreateDataset(word_list ,phoneme_list , stress = True)\n    data_without_stress = CreateDataset(word_list ,phoneme_list , stress = False)\n    return data_with_stress , data_without_stress","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:09.838853Z","iopub.execute_input":"2024-06-26T21:42:09.839199Z","iopub.status.idle":"2024-06-26T21:42:09.849654Z","shell.execute_reply.started":"2024-06-26T21:42:09.839162Z","shell.execute_reply":"2024-06-26T21:42:09.848629Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_train_with , data_train_without = crate_dataset(word_list_train ,phoneme_list_train)\ndata_test_with , data_test_without = crate_dataset(word_list_test ,phoneme_list_test)\ndata_val_with , data_val_without = crate_dataset(word_list_val ,phoneme_list_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:09.850967Z","iopub.execute_input":"2024-06-26T21:42:09.851329Z","iopub.status.idle":"2024-06-26T21:42:09.858828Z","shell.execute_reply.started":"2024-06-26T21:42:09.851294Z","shell.execute_reply":"2024-06-26T21:42:09.857898Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_bpe_train_l =  ['Ġ'.join(i[\"label\"].split()) for i in data_train_without ]\ndata_bpe_train_s =  [i[\"input\"].upper() for i in  data_test_without  ]\ndata_bpe_test_l =  ['Ġ'.join(i[\"label\"].split()) for i in data_test_without ]\ndata_bpe_test_s =  [i[\"input\"].upper() for i in  data_test_without ]\ndata_bpe_val_l = ['Ġ'.join(i[\"label\"].split()) for i in data_val_without ]\ndata_bpe_val_s = [i[\"input\"].upper() for i in data_val_without ]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:09.860170Z","iopub.execute_input":"2024-06-26T21:42:09.860534Z","iopub.status.idle":"2024-06-26T21:42:11.314152Z","shell.execute_reply.started":"2024-06-26T21:42:09.860508Z","shell.execute_reply":"2024-06-26T21:42:11.313009Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tokenizers import CharBPETokenizer\ntokenizer = CharBPETokenizer()\ndata_for_bpe = data_bpe_train_l+data_bpe_train_s+data_bpe_test_l+data_bpe_test_s+data_bpe_val_l+data_bpe_val_s\ntokenizer.train_from_iterator(data_for_bpe ,vocab_size=256)\ntokenizer.add_special_tokens([wandb.config.PAD_TOKEN,\n                              wandb.config.BOS_TOKEN,\n                              wandb.config.EOS_TOKEN,\n                              wandb.config.UNK_TOKEN])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:11.315824Z","iopub.execute_input":"2024-06-26T21:42:11.316818Z","iopub.status.idle":"2024-06-26T21:42:24.300570Z","shell.execute_reply.started":"2024-06-26T21:42:11.316776Z","shell.execute_reply":"2024-06-26T21:42:24.299664Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.save(\"/kaggle/working/bpe_256_lex\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.302094Z","iopub.execute_input":"2024-06-26T21:42:24.302451Z","iopub.status.idle":"2024-06-26T21:42:24.309029Z","shell.execute_reply.started":"2024-06-26T21:42:24.302419Z","shell.execute_reply":"2024-06-26T21:42:24.308046Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def causal_mask(size):\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.311200Z","iopub.execute_input":"2024-06-26T21:42:24.312231Z","iopub.status.idle":"2024-06-26T21:42:24.319956Z","shell.execute_reply.started":"2024-06-26T21:42:24.312198Z","shell.execute_reply":"2024-06-26T21:42:24.319128Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class Dataset_WithPudding:\n    def __init__(self, sentences , max_len: int = 32, debag=False):\n        self.ds = sentences\n        self.seq_len = max_len \n        self.debag = debag\n        self.pad_id = torch.tensor(tokenizer.encode(wandb.config.PAD_TOKEN).ids)\n\n    def __getitem__(self, idx: int):\n        \n        src_target_pair = self.ds[idx]\n        \n        src_text = src_target_pair[\"input\"]\n        tgt_text = 'Ġ'.join(src_target_pair['label'].split())\n        \n        enc_input_tokens = torch.tensor(tokenizer.encode(src_text).ids)\n        dec_input_tokens = tokenizer.encode(tgt_text).ids \n\n    \n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n        # We will only add <s>, and </s> only on the label\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n        \n        #print(enc_num_padding_tokens , dec_num_padding_tokens)\n        \n        encoder_input = torch.cat(\n            [\n                torch.tensor(tokenizer.encode(wandb.config.BOS_TOKEN).ids),\n                enc_input_tokens,\n                torch.tensor(tokenizer.encode(wandb.config.EOS_TOKEN).ids),\n                self.pad_id.repeat(enc_num_padding_tokens)\n            ],\n            dim=0,\n        )\n        \n        \n        # Add only <s> token\n        decoder_input = torch.cat(\n            [\n                torch.tensor(tokenizer.encode(wandb.config.BOS_TOKEN).ids),\n                torch.tensor(dec_input_tokens),\n                self.pad_id.repeat(dec_num_padding_tokens)\n            ],\n            dim=0,\n        )\n\n        # Add only </s> token\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                torch.tensor(tokenizer.encode(wandb.config.EOS_TOKEN).ids),\n                self.pad_id.repeat(dec_num_padding_tokens),\n            ],\n            dim=0,\n        )\n        \n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return {\n            \"encoder_input\": encoder_input,  # (seq_len)\n            \"decoder_input\": decoder_input,  # (seq_len)\n            \"encoder_mask\": (encoder_input != self.pad_id).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n            \"decoder_mask\": (decoder_input != self.pad_id).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n            \"label\": label,  # (seq_len)\n            \"src_text\": src_text.upper(), # (seq_len)\n            \"tgt_text\": src_target_pair['label'] # (seq_len)\n        }\n\n\n    def __len__(self) -> int:\n        return len(self.ds)\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.321093Z","iopub.execute_input":"2024-06-26T21:42:24.322447Z","iopub.status.idle":"2024-06-26T21:42:24.345140Z","shell.execute_reply.started":"2024-06-26T21:42:24.322410Z","shell.execute_reply":"2024-06-26T21:42:24.344419Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def collate_fn(bath):\n    encoder_input = torch.stack([i[\"encoder_input\"].to(device) for i in bath])\n    decoder_input =  torch.stack([i[\"decoder_input\"].to(device) for i in bath])\n    encoder_mask =  torch.stack([i[\"encoder_mask\"].to(device) for i in bath])\n    decoder_mask = torch.stack([i[\"decoder_mask\"].to(device) for i in bath])\n    label = torch.stack( [i[\"label\"] for i in bath] )\n    src_text = [i[\"src_text\"] for i in bath]\n    tgt_text = [i[\"tgt_text\"] for i in bath]\n    \n    return {\n            \"encoder_input\": encoder_input,  # (seq_len)\n            \"decoder_input\": decoder_input,  # (seq_len)\n            \"encoder_mask\": encoder_mask, # (1, 1, seq_len)\n            \"decoder_mask\": decoder_mask, # (1, seq_len) & (1, seq_len, seq_len),\n            \"label\": label,  # (seq_len)\n            \"src_text\": src_text, # (seq_len)\n            \"tgt_text\": tgt_text, # (seq_len) \n           }","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.346326Z","iopub.execute_input":"2024-06-26T21:42:24.346659Z","iopub.status.idle":"2024-06-26T21:42:24.360527Z","shell.execute_reply.started":"2024-06-26T21:42:24.346628Z","shell.execute_reply":"2024-06-26T21:42:24.359647Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"dataset_train = Dataset_WithPudding(data_train_without)\ndataset_test = Dataset_WithPudding(data_test_without)\ndataset_val = Dataset_WithPudding(data_val_without)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.361882Z","iopub.execute_input":"2024-06-26T21:42:24.362241Z","iopub.status.idle":"2024-06-26T21:42:24.394349Z","shell.execute_reply.started":"2024-06-26T21:42:24.362210Z","shell.execute_reply":"2024-06-26T21:42:24.393419Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def decode_form_G(tokens):\n    return ''.join(tokens).split('Ġ')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.395616Z","iopub.execute_input":"2024-06-26T21:42:24.395945Z","iopub.status.idle":"2024-06-26T21:42:24.401292Z","shell.execute_reply.started":"2024-06-26T21:42:24.395915Z","shell.execute_reply":"2024-06-26T21:42:24.400130Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"batch_size = wandb.config.BATCH_SIZE\n\ntrain_dataloader = DataLoader(\n        dataset_train,\n        shuffle=True,\n        batch_size=batch_size,\n        drop_last=True,\n        collate_fn=collate_fn)\n\nval_dataloader = DataLoader(\n        dataset_val,\n        batch_size=batch_size,\n        collate_fn=collate_fn)\n\ntest_dataloader =DataLoader(\n        dataset_test,\n        batch_size=batch_size,\n        collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.402458Z","iopub.execute_input":"2024-06-26T21:42:24.402763Z","iopub.status.idle":"2024-06-26T21:42:24.411835Z","shell.execute_reply.started":"2024-06-26T21:42:24.402734Z","shell.execute_reply":"2024-06-26T21:42:24.410922Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.412902Z","iopub.execute_input":"2024-06-26T21:42:24.413237Z","iopub.status.idle":"2024-06-26T21:42:24.422954Z","shell.execute_reply.started":"2024-06-26T21:42:24.413204Z","shell.execute_reply":"2024-06-26T21:42:24.422231Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadSelfAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.depth = d_model // num_heads\n        \n        self.wq = nn.Linear(d_model, d_model)\n        self.wk = nn.Linear(d_model, d_model)\n        self.wv = nn.Linear(d_model, d_model)\n        \n        self.fc = nn.Linear(d_model, d_model)\n        \n    def split_heads(self, x, batch_size):\n        x = x.view(batch_size, -1, self.num_heads, self.depth)\n        return x.permute(0, 2, 1, 3)\n    \n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0)\n        \n        q = self.split_heads(self.wq(q), batch_size)\n        k = self.split_heads(self.wk(k), batch_size)\n        v = self.split_heads(self.wv(v), batch_size)\n        \n        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn = F.softmax(scores, dim=-1)\n        \n        out = torch.matmul(attn, v)\n        out = out.permute(0, 2, 1, 3).contiguous()\n        out = out.view(batch_size, -1, self.d_model)\n        \n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.424114Z","iopub.execute_input":"2024-06-26T21:42:24.424378Z","iopub.status.idle":"2024-06-26T21:42:24.439029Z","shell.execute_reply.started":"2024-06-26T21:42:24.424354Z","shell.execute_reply":"2024-06-26T21:42:24.438129Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class FeedForwardNetwork(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(FeedForwardNetwork, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.440082Z","iopub.execute_input":"2024-06-26T21:42:24.440401Z","iopub.status.idle":"2024-06-26T21:42:24.450312Z","shell.execute_reply.started":"2024-06-26T21:42:24.440364Z","shell.execute_reply":"2024-06-26T21:42:24.449292Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n        \n        self.layernorm1 = nn.LayerNorm(d_model)\n        self.layernorm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.layernorm1(x + self.dropout(attn_output))\n        \n        ffn_output = self.ffn(x)\n        x = self.layernorm2(x + self.dropout(ffn_output))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.451456Z","iopub.execute_input":"2024-06-26T21:42:24.452203Z","iopub.status.idle":"2024-06-26T21:42:24.475829Z","shell.execute_reply.started":"2024-06-26T21:42:24.452172Z","shell.execute_reply":"2024-06-26T21:42:24.474859Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadSelfAttention(d_model, num_heads)\n        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n        \n        self.layernorm1 = nn.LayerNorm(d_model)\n        self.layernorm2 = nn.LayerNorm(d_model)\n        self.layernorm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        self_attn_output = self.self_attn(q=x, k=x, v=x, mask=tgt_mask)\n        x = self.layernorm1(x + self.dropout(self_attn_output))\n        \n        cross_attn_output = self.cross_attn(q=x, k=enc_output, v=enc_output, mask=src_mask)\n        x = self.layernorm2(x + self.dropout(cross_attn_output))\n        \n        ffn_output = self.ffn(x)\n        x = self.layernorm3(x + self.dropout(ffn_output))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.477058Z","iopub.execute_input":"2024-06-26T21:42:24.477620Z","iopub.status.idle":"2024-06-26T21:42:24.487552Z","shell.execute_reply.started":"2024-06-26T21:42:24.477589Z","shell.execute_reply":"2024-06-26T21:42:24.486602Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, num_heads=8, num_encoder_layers=3, num_decoder_layers=3, d_ff=2048, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n        \n        self.pos_embedding = PositionalEncoding(d_model , wandb.config.MAX_LEN)\n        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_decoder_layers)])\n        \n        self.fc_out = nn.Linear(d_model, target_vocab_size)\n    \n    def encode(self, src, src_mask):\n        src = self.pos_embedding(self.encoder_embedding(src))\n        for layer in self.encoder_layers:\n            src = layer(src, src_mask)\n        return src\n    \n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        tgt = self.pos_embedding(self.decoder_embedding(tgt))\n        for layer in self.decoder_layers:\n            tgt = layer(tgt, memory, src_mask, tgt_mask)\n        return tgt\n    \n    def forward(self, src, tgt, src_mask, tgt_mask):\n        memory = self.encode(src, src_mask)\n        output = self.decode(memory, src_mask, tgt, tgt_mask)\n        output = self.fc_out(output)\n        return output\n\n    def decode_from_G_out(self ,seq):\n        return ''.join(seq).split('Ġ')\n    \n    \n    def pred(self, srs):\n        \n        model.eval()\n        with torch.no_grad():\n            seq = ''.join(list(srs.upper()))\n\n            enc_input_tokens = tokenizer.encode(seq).ids\n            pad_id =torch.tensor(tokenizer.encode(wandb.config.PAD_TOKEN).ids)\n            enc_num_padding_tokens = 32 - len(enc_input_tokens) - 2\n            encoder_input = torch.cat(\n                        [\n                            torch.tensor(tokenizer.encode(wandb.config.BOS_TOKEN).ids),\n                            torch.tensor(enc_input_tokens),\n                            torch.tensor(tokenizer.encode(wandb.config.EOS_TOKEN).ids),\n                            pad_id.repeat(enc_num_padding_tokens)\n                        ],\n                        dim=0,\n                    )\n\n            encoder_mask = (encoder_input != pad_id).unsqueeze(0).unsqueeze(0).int()\n\n            label   = beam_search(model=model.to(device),\n                                                     src=encoder_input.to(device),\n                                                     src_mask=encoder_mask.to(device),\n                                                     max_len=wandb.config.MAX_LEN,\n                                                     start_symbol=tokenizer.encode(wandb.config.BOS_TOKEN).ids[0],\n                                                     trg=None ,\n                                                     metricks=False)\n            model.train()\n        return self.decode_from_G_out( tokenizer.decode(label[0].tolist()) )\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.488858Z","iopub.execute_input":"2024-06-26T21:42:24.489596Z","iopub.status.idle":"2024-06-26T21:42:24.508818Z","shell.execute_reply.started":"2024-06-26T21:42:24.489565Z","shell.execute_reply":"2024-06-26T21:42:24.507897Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def bleu_score(word,test_pronunciation):\n    smooth = SmoothingFunction().method1\n    return sentence_bleu(word, test_pronunciation, smoothing_function=smooth)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.509989Z","iopub.execute_input":"2024-06-26T21:42:24.510710Z","iopub.status.idle":"2024-06-26T21:42:24.524492Z","shell.execute_reply.started":"2024-06-26T21:42:24.510678Z","shell.execute_reply":"2024-06-26T21:42:24.523535Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def phoneme_error_rate(predicted, target):\n    m, n = len(predicted), len(target)\n    if m == 0:\n        return float(n)\n    if n == 0:\n        return float(m)\n\n    d = np.zeros((m + 1, n + 1), dtype=int)\n\n    for i in range(m + 1):\n        d[i][0] = i\n    for j in range(n + 1):\n        d[0][j] = j\n\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if predicted[i - 1] == target[j - 1] else 1\n            d[i][j] = min(d[i - 1][j] + 1,    # Deletion\n                          d[i][j - 1] + 1,    # Insertion\n                          d[i - 1][j - 1] + cost)  # Substitution\n            if i > 1 and j > 1 and predicted[i - 1] == target[j - 2] and predicted[i - 2] == target[j - 1]:\n                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)  \n\n    num_errors = d[m][n]\n    num_phonemes = len(target)\n\n    return num_errors / num_phonemes\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.530013Z","iopub.execute_input":"2024-06-26T21:42:24.530303Z","iopub.status.idle":"2024-06-26T21:42:24.543633Z","shell.execute_reply.started":"2024-06-26T21:42:24.530280Z","shell.execute_reply":"2024-06-26T21:42:24.542775Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def word_error_rate(predicted, target):\n    return 0 if predicted == target else 1","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.546973Z","iopub.execute_input":"2024-06-26T21:42:24.547481Z","iopub.status.idle":"2024-06-26T21:42:24.557940Z","shell.execute_reply.started":"2024-06-26T21:42:24.547450Z","shell.execute_reply":"2024-06-26T21:42:24.557110Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def beam_search(model, src, src_mask, max_len, start_symbol, trg, metricks:False ,beam_size=5):\n    src = src.unsqueeze(0)  \n    src_mask = src_mask.unsqueeze(0) \n    memory = model.encode(src, src_mask)\n    \n    beams = [(torch.ones(1, 1).fill_(start_symbol).type_as(src.data), 0.0)]  # (последовательность, score)\n\n    for _ in range(max_len - 1):\n        new_beams = []\n        for seq, score in beams:\n            tgt_mask = causal_mask(seq.size(1)).unsqueeze(0).to(device)\n            out = model.decode(memory, src_mask, seq, tgt_mask)\n            prob = model.fc_out(out[:, -1])\n            log_prob = F.log_softmax(prob, dim=-1)\n            \n            top_log_prob, top_indices = log_prob.topk(beam_size)\n\n            for i in range(beam_size):\n                next_seq = torch.cat([seq, torch.ones(1, 1).type_as(src.data).fill_(top_indices[0, i])], dim=1)\n                new_score = score + top_log_prob[0, i].item()\n                new_beams.append((next_seq, new_score))\n\n\n        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n        \n        if beams[0][0][0][-1].item() == tokenizer.encode(wandb.config.EOS_TOKEN).ids[0]:\n            break\n    \n    best_seq, best_score = beams[0]\n    \n    if metricks:\n        pred = decode_form_G(tokenizer.decode(best_seq[0].tolist()))    \n        per = phoneme_error_rate( pred  , trg.split() )\n        wer = word_error_rate( pred  , trg.split() )\n        bleu = bleu_score(pred  , trg.split())\n        \n        return best_seq , per , wer ,bleu\n    return best_seq\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.559610Z","iopub.execute_input":"2024-06-26T21:42:24.560347Z","iopub.status.idle":"2024-06-26T21:42:24.575908Z","shell.execute_reply.started":"2024-06-26T21:42:24.560313Z","shell.execute_reply":"2024-06-26T21:42:24.575049Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def model_eval(model , dataloader , beam=True):\n    model.eval()\n    with torch.no_grad():\n        per_list = []\n        wer_list = []\n        bleu_list = []\n        for batch in tqdm( dataloader ):\n            per_batch=0.0\n            wer_batch=0.0\n            bleu_batch=0.0\n            for idx in range(len(batch)):\n                encoder_input = batch['encoder_input'][idx].to(device)\n                encoder_mask = batch['encoder_mask'][idx].to(device)\n                trg = batch['tgt_text'][idx]\n                if beam:\n                    label ,per ,wer,bleu  = beam_search( model=model,\n                                                         src=encoder_input,\n                                                         src_mask=encoder_mask,\n                                                         max_len=wandb.config.MAX_LEN,\n                                                         start_symbol=tokenizer.encode(wandb.config.BOS_TOKEN).ids[0],\n                                                         trg=trg ,\n                                                         metricks=True)\n                per_batch+=per\n                wer_batch+=wer\n                bleu_batch+=bleu\n\n            per_list.append(per_batch / len(batch))\n            wer_list.append(wer_batch / len(batch))\n            bleu_list.append(bleu_batch / len(batch))\n\n    return sum(per_list)/len(dataloader) , sum(wer_list)/len(dataloader),  sum(bleu_list)/len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.576955Z","iopub.execute_input":"2024-06-26T21:42:24.577238Z","iopub.status.idle":"2024-06-26T21:42:24.589774Z","shell.execute_reply.started":"2024-06-26T21:42:24.577215Z","shell.execute_reply":"2024-06-26T21:42:24.588928Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"best_per = 10\nbest_per_test = 10\nnum_cycles = 4","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.591030Z","iopub.execute_input":"2024-06-26T21:42:24.591439Z","iopub.status.idle":"2024-06-26T21:42:24.605363Z","shell.execute_reply.started":"2024-06-26T21:42:24.591369Z","shell.execute_reply":"2024-06-26T21:42:24.604430Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model = TransformerBlock(input_vocab_size=tokenizer.get_vocab_size(),\n                        target_vocab_size=tokenizer.get_vocab_size(),\n                         num_encoder_layers=wandb.config.NUM,\n                        num_decoder_layers=wandb.config.NUM,\n                         num_heads=wandb.config.NUM_HEADS,\n                         d_model = wandb.config.D_MODEL,\n                         d_ff = wandb.config.D_FF).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.LR , betas=(0.9, 0.98))\nnum_epohs = wandb.config.NUM_EPOHS\nnum_training_steps = len(train_dataloader) * num_epohs\nnum_warmup_steps = int(0.1 * num_training_steps)\nloss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer.encode(wandb.config.PAD_TOKEN).ids[0] , label_smoothing=0.1 ).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:24.606557Z","iopub.execute_input":"2024-06-26T21:42:24.606779Z","iopub.status.idle":"2024-06-26T21:42:25.137966Z","shell.execute_reply.started":"2024-06-26T21:42:24.606759Z","shell.execute_reply":"2024-06-26T21:42:25.136902Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer=optimizer ,\n                                                               num_warmup_steps=num_warmup_steps,\n                                                               num_training_steps=num_training_steps,\n                                                               num_cycles=num_cycles)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:25.139143Z","iopub.execute_input":"2024-06-26T21:42:25.139419Z","iopub.status.idle":"2024-06-26T21:42:25.145042Z","shell.execute_reply.started":"2024-06-26T21:42:25.139396Z","shell.execute_reply":"2024-06-26T21:42:25.144213Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def masked(seq):\n    pad_list = torch.tensor( [(tokenizer.encode(wandb.config.PAD_TOKEN).ids[0]) for _ in range(len(seq))]).to(device)\n    count_pad = (pad_list == seq).int().sum().item()\n    total_length = wandb.config.MAX_LEN - count_pad - 2\n    num_zeros = int((total_length*0.15)//1)\n    # Задаем случайные индексы для установки 1 вместо 0\n    indices = np.random.choice(np.arange(1, total_length+1), num_zeros, replace=False)\n    seq[indices] = tokenizer.encode(wandb.config.UNK_TOKEN).ids[0]\n    return seq","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:25.146325Z","iopub.execute_input":"2024-06-26T21:42:25.146600Z","iopub.status.idle":"2024-06-26T21:42:25.158213Z","shell.execute_reply.started":"2024-06-26T21:42:25.146575Z","shell.execute_reply":"2024-06-26T21:42:25.157005Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def masked_batch(batch):\n    return  torch.stack([masked(seq) for seq in batch])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:25.159395Z","iopub.execute_input":"2024-06-26T21:42:25.160092Z","iopub.status.idle":"2024-06-26T21:42:25.167918Z","shell.execute_reply.started":"2024-06-26T21:42:25.160042Z","shell.execute_reply":"2024-06-26T21:42:25.166986Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"for epoch in range(wandb.config.NUM_EPOHS):\n    model.train()\n    loss_epoch = 0.0\n    for batch_idx, batch in tqdm(enumerate(train_dataloader), desc=f\"Processing Epoch {epoch:02d}\"):\n        encoder_input = batch['encoder_input'].to(device)  # (B, seq_len) with unk \n        decoder_input = batch['decoder_input'].to(device)  # (B, seq_len)\n        encoder_mask = batch['encoder_mask'].to(device)  # (B, 1, 1, seq_len)\n        decoder_mask = batch['decoder_mask'].to(device)  # (B, 1, seq_len, seq_len)\n        label = batch['label'].to(device)  # (B, seq_len)\n\n        output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n        loss = loss_fn(output.view(-1, tokenizer.get_vocab_size()), label.view(-1))\n        loss_epoch += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()  \n        scheduler.step()\n\n\n    # Evaluate the model\n    per, wer, bleu = model_eval(model, val_dataloader)\n\n    if per < best_per:\n        best_per = per\n        per_test, wer_test, bleu_test = model_eval(model, test_dataloader)\n        \n        if per_test < best_per_test:\n            best_per_test = per_test\n            torch.save(model.state_dict(), f\"/kaggle/working/model/model{per:.4f}.pt\")\n\n    print(f\"PER: {per*100:.2f} | Loss: {loss_epoch/len(train_dataloader):.4f} | WER: {wer*100:.2f} | BLEU: {bleu:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n    wandb.log({ \"PER\": per*100, \n                \"loss\": loss_epoch/len(train_dataloader), \n                \"WER\": wer*100, \n                \"BLEU\": bleu, \n                \"LR\": scheduler.get_last_lr()[0]})","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:42:25.169348Z","iopub.execute_input":"2024-06-26T21:42:25.169727Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Epoch 00: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94b504f7bc4409ab44e2540d363ecbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9e015649c8e4104b2702b95444c5bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0241e33e88b4a90b37e08db754bd74d"}},"metadata":{}},{"name":"stdout","text":"PER: 28.84 | Loss: 3.4655 | WER: 87.50 | BLEU: 0.0551 | LR: 0.000075\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Epoch 01: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"924546e696ac4cd39d6df04e28e675d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36df457c00f4b4dbc76246c19524d3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f440c15327449d4ad0c91f882996ba0"}},"metadata":{}},{"name":"stdout","text":"PER: 16.72 | Loss: 1.5216 | WER: 67.86 | BLEU: 0.0558 | LR: 0.000150\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Epoch 02: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2eef4c9e1a64cd7b92f0cc354556272"}},"metadata":{}}]},{"cell_type":"code","source":"per, wer , bleu = model_eval(model , test_dataloader)\nper*100, wer*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, \"/kaggle/working/model/modellast.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}